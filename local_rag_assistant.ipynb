{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472fe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9ad15",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5283636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and Processing Documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Loading and Processing Documents...\")\n",
    "try:\n",
    "    with open('project_1_publications.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'project_1_publications.json' not found.\")\n",
    "    print(\"Please make sure the JSON file is in the same directory as this script.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the publication descriptions and create LangChain Document objects\n",
    "# This follows the principle of reading our files first.\n",
    "documents = []\n",
    "\n",
    "for item in data:\n",
    "    content = item.get(\"publication_description\", \"\")\n",
    "    metadata = {\"title\": item.get(\"title\", \"No Title\")}\n",
    "    documents.append(Document(page_content=content, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377c9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 1207 document chunks.\n"
     ]
    }
   ],
   "source": [
    "# Divide the texts into chunks so they can be fed into your embedding model.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded and split {len(docs)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de60570",
   "metadata": {},
   "source": [
    "### Step 2: Create Embeddings and Store in Vector Database (FAISS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Creating Embeddings and Storing in FAISS...\n",
      "Embeddings created and stored in FAISS.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nStep 2: Creating Embeddings and Storing in FAISS...\")\n",
    "# Initialize Ollama embeddings using the Mistral model\n",
    "# This step embeds the chunked texts into vectors.\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "\n",
    "# Create a FAISS vector store from the document chunks and their embeddings.\n",
    "# This pushes the vectors and text to the database.\n",
    "# FAISS is a local vector store, so it saves the index to your disk.\n",
    "vector = FAISS.from_documents(docs, ollama_embeddings)\n",
    "print(\"Embeddings created and stored in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2256f1",
   "metadata": {},
   "source": [
    "### Step 3: Initialize the LLM and Create the RAG Chain |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452617ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Initializing LLM and Creating RAG Chain...\n",
      "RAG chain created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gouri\\AppData\\Local\\Temp\\ipykernel_20128\\3396880923.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nStep 3: Initializing LLM and Creating RAG Chain...\")\n",
    "# Initialize the Ollama LLM with the Mistral model\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Create a prompt template. This structures the input for the LLM.\n",
    "# The goal is to pass the answers from the vector database to your LLM.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create the main chain that combines document retrieval and question answering\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the retrieval chain. This component will retrieve documents from the\n",
    "# vector store before passing them to the document_chain.\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "print(\"RAG chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85be07",
   "metadata": {},
   "source": [
    "# Step 4: Ask Questions and Get Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553d821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Ready to Answer Questions!\n",
      "Type 'exit' to quit.\n",
      "\n",
      "Assistant's Answer:\n",
      " In the provided context, there is no information about defining a custom agent for a specific task such as a game or a chatbot. However, if we are talking about a Neural Network model (like NBeats), here's how you might define a custom agent:\n",
      "\n",
      "1. Identify the problem you want to solve with your custom agent. This could be anything from playing a game like chess or Go, to controlling a robot, to generating text in a chatbot.\n",
      "\n",
      "2. Choose an appropriate architecture for your model based on the problem you're trying to solve. For instance, if it's a sequence-to-sequence task like text generation, you might choose an LSTM or Transformer architecture. If it's a game with a large state space, you might consider using a deep reinforcement learning model.\n",
      "\n",
      "3. Prepare your dataset and labels for training the agent. This might involve collecting data from the environment, cleaning the data, and possibly preprocessing it in some way to make it suitable for the neural network.\n",
      "\n",
      "4. Train the model on your dataset using an appropriate loss function and optimizer. This might involve splitting your data into a training set and a validation set, and using techniques like cross-validation to ensure that your model is learning effectively from the data.\n",
      "\n",
      "5. Test your trained agent on new data to evaluate its performance. This could involve running simulations, playing games, or generating text. You'll want to measure some metrics of performance to determine how well your custom agent is doing compared to other possible solutions.\n",
      "\n",
      "6. Iterate on your model by fine-tuning hyperparameters, adding layers to the network, changing the loss function or optimizer, or collecting more data to improve its performance if necessary.\n",
      "\n",
      "This process can be complex and time-consuming, but with careful design and experimentation, it's possible to create a custom agent that is effective at solving the problem you've defined.\n",
      "Exiting assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nStep 4: Ready to Answer Questions!\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\\nYour Question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Invoke the chain with the user's question\n",
    "        response = retrieval_chain.invoke({\"input\": user_input})\n",
    "\n",
    "        # Print the answer\n",
    "        print(\"\\nAssistant's Answer:\")\n",
    "        print(response[\"answer\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e20677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant's Answer:\n",
      " The provided context does not explicitly explain the role of memory in LangGraph as it only discusses ChatGPT's use of memory. However, considering that Langchain-OpenAI and knowledge graphs share similarities with how ChatGPT functions (by using connections between entities), we can infer that memory in LangGraph would play a role in enabling the platform to remember relationships between various entities (films, actors, directors, etc.) and use this information to answer complex queries efficiently.\n",
      "\n",
      "For a more accurate answer, it would be best to consult documentation or research about Langchain-OpenAI specifically.\n"
     ]
    }
   ],
   "source": [
    "user_input_2 = \"Whatâ€™s the role of memory in LangGraph?\"\n",
    "response_2 = retrieval_chain.invoke({\"input\": user_input_2})\n",
    "\n",
    "print(\"\\nAssistant's Answer:\")\n",
    "print(response_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14323f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
