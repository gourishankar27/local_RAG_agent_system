{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472fe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9ad15",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5283636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and Processing Documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Loading and Processing Documents...\")\n",
    "try:\n",
    "    with open('project_1_publications.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'project_1_publications.json' not found.\")\n",
    "    print(\"Please make sure the JSON file is in the same directory as this script.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555d4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the publication descriptions and create LangChain Document objects\n",
    "# This follows the principle of reading our files first.\n",
    "documents = []\n",
    "\n",
    "for item in data:\n",
    "    content = item.get(\"publication_description\", \"\")\n",
    "    metadata = {\"title\": item.get(\"title\", \"No Title\")}\n",
    "    documents.append(Document(page_content=content, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377c9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 1207 document chunks.\n"
     ]
    }
   ],
   "source": [
    "# Divide the texts into chunks so they can be fed into your embedding model.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded and split {len(docs)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de60570",
   "metadata": {},
   "source": [
    "### Step 2: Create Embeddings and Store in Vector Database (FAISS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799b0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Creating Embeddings and Storing in FAISS...\n",
      "Embeddings created and stored in FAISS.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nStep 2: Creating Embeddings and Storing in FAISS...\")\n",
    "# Initialize Ollama embeddings using the Mistral model\n",
    "# This step embeds the chunked texts into vectors.\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "\n",
    "# Create a FAISS vector store from the document chunks and their embeddings.\n",
    "# This pushes the vectors and text to the database.\n",
    "# FAISS is a local vector store, so it saves the index to your disk.\n",
    "vector = FAISS.from_documents(docs, ollama_embeddings)\n",
    "print(\"Embeddings created and stored in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2256f1",
   "metadata": {},
   "source": [
    "### Step 3: Initialize the LLM and Create the RAG Chain |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452617ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Initializing LLM and Creating RAG Chain...\n",
      "RAG chain created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gouri\\AppData\\Local\\Temp\\ipykernel_7212\\1089255286.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nStep 3: Initializing LLM and Creating RAG Chain...\")\n",
    "# Initialize the Ollama LLM with the Mistral model\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Create a prompt template. This structures the input for the LLM.\n",
    "# The goal is to pass the answers from the vector database to your LLM.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create the main chain that combines document retrieval and question answering\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the retrieval chain. This component will retrieve documents from the\n",
    "# vector store before passing them to the document_chain.\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "print(\"RAG chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85be07",
   "metadata": {},
   "source": [
    "# Step 4: Ask Questions and Get Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a553d821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Ready to Answer Questions!\n",
      "Type 'exit' to quit.\n",
      "\n",
      "Assistant's Answer:\n",
      " I'm sorry for any confusion, but there was no mention of QLoRA in the provided context. It appears that the conversation revolves around the topics of reproducibility, knowledge graphs, and graph databases, as well as CLIP (Contrastive Language-Image Pretraining). If you have more information or a different context where QLoRA is mentioned, I'd be happy to help explain it!\n",
      "Exiting assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nStep 4: Ready to Answer Questions!\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"\\nYour Question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Invoke the chain with the user's question\n",
    "        response = retrieval_chain.invoke({\"input\": user_input})\n",
    "\n",
    "        # Print the answer\n",
    "        print(\"\\nAssistant's Answer:\")\n",
    "        print(response[\"answer\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e20677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant's Answer:\n",
      " The provided context does not explicitly explain the role of memory in LangGraph as it only discusses ChatGPT's use of memory. However, considering that Langchain-OpenAI and knowledge graphs share similarities with how ChatGPT functions (by using connections between entities), we can infer that memory in LangGraph would play a role in enabling the platform to remember relationships between various entities (films, actors, directors, etc.) and use this information to answer complex queries efficiently.\n",
      "\n",
      "For a more accurate answer, it would be best to consult documentation or research about Langchain-OpenAI specifically.\n"
     ]
    }
   ],
   "source": [
    "user_input_2 = \"Whatâ€™s the role of memory in LangGraph?\"\n",
    "response_2 = retrieval_chain.invoke({\"input\": user_input_2})\n",
    "\n",
    "print(\"\\nAssistant's Answer:\")\n",
    "print(response_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14323f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
